# Function Calling ÂäüËÉΩÂÆûÁé∞ÊñáÊ°£

## üìã Ê¶ÇËø∞

Êú¨ÊñáÊ°£ËÆ∞ÂΩï‰∫Ü Yanfeng AI Task ÈõÜÊàê‰∏≠ Function CallingÔºàÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºâÂäüËÉΩÁöÑÂÆåÊï¥ÂÆûÁé∞ËøáÁ®ãÔºåÂåÖÊã¨ÈóÆÈ¢òËØäÊñ≠„ÄÅËß£ÂÜ≥ÊñπÊ°àÂíåÊµãËØïÈ™åËØÅ„ÄÇ

## üéØ ÈóÆÈ¢òÊèèËø∞

### ÂàùÂßãÈóÆÈ¢ò
Áî®Êà∑ÂèëÁé∞ÂêåÊ†∑ÁöÑÊü•ËØ¢ËÆæÂ§á‰ø°ÊÅØÁöÑÈóÆÈ¢òÔºö
- ‚úÖ **Gemini ÈõÜÊàê**ÔºöÂèØ‰ª•ÊàêÂäüË∞ÉÁî® `GetLiveContext` Â∑•ÂÖ∑Êü•ËØ¢ËÆæÂ§áÁä∂ÊÄÅ
- ‚ùå **YanfengAI ÈõÜÊàê**ÔºöÊó†Ê≥ïË∞ÉÁî®Â∑•ÂÖ∑ÔºåÊó†Ê≥ïÊü•ËØ¢ËÆæÂ§á‰ø°ÊÅØ

**ÊµãËØïÁî®‰æã**Ôºö
```
Áî®Êà∑ÈóÆÈ¢òÔºöÁé∞Âú®‰π¶ÊàøÊ∏©Â∫¶Â§öÂ∞ëÔºü
ÊúüÊúõË°å‰∏∫ÔºöË∞ÉÁî® GetLiveContext Â∑•ÂÖ∑Ëé∑ÂèñËÆæÂ§áÁä∂ÊÄÅÔºåËøîÂõûÊ∏©Â∫¶‰ø°ÊÅØ
ÂÆûÈôÖË°å‰∏∫ÔºöÊó†Ê≥ïË∞ÉÁî®Â∑•ÂÖ∑ÔºåÊó†Ê≥ïËé∑ÂèñËÆæÂ§á‰ø°ÊÅØ
```

### Ê†πÊú¨ÂéüÂõ†ÂàÜÊûê

ÁªèËøáÊ∑±ÂÖ•Ë∞ÉÊü•ÔºåÂèëÁé∞‰ª•‰∏ãÈóÆÈ¢òÔºö

1. **Áº∫Â∞ë Function Calling ÂÆûÁé∞**
   - `entity.py` ‰∏≠Ê≤°ÊúâÂÆûÁé∞ OpenAI ÂÖºÂÆπÁöÑ function calling Âæ™ÁéØ
   - Ê≤°ÊúâÊèêÂèñÂíåÊ†ºÂºèÂåñ HA Êèê‰æõÁöÑÂ∑•ÂÖ∑ÔºàtoolsÔºâ
   - Ê≤°ÊúâÂ§ÑÁêÜÊ®°ÂûãËøîÂõûÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®ËØ∑Ê±Ç

2. **Ê∂àÊÅØÂ§ÑÁêÜ‰∏çÂÆåÊï¥**
   - Ê≤°ÊúâÊ≠£Á°ÆÂ§ÑÁêÜ `ToolResultContent` Á±ªÂûãÁöÑÊ∂àÊÅØ
   - Áº∫Â∞ëÂ∑•ÂÖ∑ÊâßË°åÁªìÊûúÂõû‰º†ÁªôÊ®°ÂûãÁöÑÈÄªËæë

3. **API ÂèÇÊï∞Áº∫Â§±**
   - `helpers.py` ÁöÑ `generate_text()` ÊñπÊ≥ïÊ≤°ÊúâÊîØÊåÅ `tools` ÂèÇÊï∞

## üîç Ë∞ÉËØïËøáÁ®ã

### Èò∂ÊÆµ 1ÔºöÈ™åËØÅ ModelScope API ÊîØÊåÅ

**ÈóÆÈ¢ò**ÔºöÈúÄË¶ÅÁ°ÆËÆ§ ModelScope API ÊòØÂê¶ÊîØÊåÅ OpenAI ÂÖºÂÆπÁöÑ function calling

**ÊñπÊ≥ï**ÔºöÂàõÂª∫ÊµãËØïËÑöÊú¨ `test_function_calling.py`

**ÊµãËØïÁªìÊûú**Ôºö
```
‚úÖ ÊµãËØïÈÄöËøá! ModelScope API ÂÆåÂÖ®ÊîØÊåÅ Function Calling!

ÊµãËØïÊµÅÁ®ãÔºö
1. ÂèëÈÄÅÂ∏¶Â∑•ÂÖ∑ÂÆö‰πâÁöÑËØ∑Ê±Ç
2. Ê®°ÂûãËøîÂõûÂ∑•ÂÖ∑Ë∞ÉÁî®ËØ∑Ê±Ç
3. ÊâßË°åÂ∑•ÂÖ∑ÔºàÊ®°ÊãüÔºâ
4. ÂèëÈÄÅÂ∑•ÂÖ∑ÁªìÊûúÂõûÊ®°Âûã
5. Ê®°ÂûãËøîÂõûÊúÄÁªàÁ≠îÊ°à

ÁªìËÆ∫ÔºöModelScope API (https://api-inference.modelscope.cn/) ÂÆåÂÖ®ÂÖºÂÆπ OpenAI function calling Ê†ºÂºè
```

### Èò∂ÊÆµ 2ÔºöÂèÇËÄÉ Google Generative AI ÂÆûÁé∞

Á†îÁ©∂‰∫Ü Google Generative AI ÈõÜÊàêÁöÑÂÆûÁé∞Ê®°ÂºèÔºö
- Â∑•ÂÖ∑ÊèêÂèñÂíåÊ†ºÂºèÂåñ
- Â∑•ÂÖ∑Ë∞ÉÁî®Âæ™ÁéØÔºàËø≠‰ª£Â§ÑÁêÜÔºâ
- Ê∂àÊÅØÂéÜÂè≤ÁÆ°ÁêÜ
- ÈîôËØØÂ§ÑÁêÜ

### Èò∂ÊÆµ 3ÔºöÂÆûÁé∞ Function Calling

Âü∫‰∫éÁ†îÁ©∂ÁªìÊûúÔºåÂÆûÁé∞‰∫ÜÂÆåÊï¥ÁöÑ function calling ÊîØÊåÅ„ÄÇ

## üõ†Ô∏è ‰ª£Á†ÅÊõ¥Êîπ

### 1. `helpers.py` - Ê∑ªÂä†Â∑•ÂÖ∑ÂèÇÊï∞ÊîØÊåÅ

**Êñá‰ª∂**Ôºö`custom_components/yanfeng_ai_task/helpers.py`

**Êõ¥Êîπ**ÔºöÂú® `generate_text()` ÊñπÊ≥ï‰∏≠Ê∑ªÂä† `tools` Âíå `tool_choice` ÂèÇÊï∞

```python
async def generate_text(
    self,
    model: str,
    messages: list[dict[str, Any]],
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_tokens: int = 2048,
    stream: bool = False,
    tools: list[dict[str, Any]] | None = None,  # Êñ∞Â¢û
    tool_choice: str = "auto",                   # Êñ∞Â¢û
) -> dict[str, Any]:
    """Generate text using ModelScope API-Inference with optional function calling."""

    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "top_p": top_p,
        "max_tokens": max_tokens,
    }

    # Add tools if provided (OpenAI-compatible function calling)
    if tools:
        payload["tools"] = tools
        payload["tool_choice"] = tool_choice
        LOGGER.debug("Added %d tools to payload with tool_choice=%s", len(tools), tool_choice)

    # ... ÂÖ∂‰Ωô‰ª£Á†Å‰øùÊåÅ‰∏çÂèò
```

### 2. `entity.py` - ÂÆûÁé∞ÂÆåÊï¥ÁöÑ Function Calling Âæ™ÁéØ

**Êñá‰ª∂**Ôºö`custom_components/yanfeng_ai_task/entity.py`

#### 2.1 Ê∑ªÂä†ÂØºÂÖ•ÂíåÂ∏∏Èáè

```python
import voluptuous as vol
from voluptuous_openapi import convert
from homeassistant.helpers import llm

# Max number of tool iterations to prevent infinite loops
MAX_TOOL_ITERATIONS = 10
```

#### 2.2 Ê∑ªÂä†Â∑•ÂÖ∑Ê†ºÂºèÂåñÂáΩÊï∞

```python
def _format_tool(tool: llm.Tool, custom_serializer: Any | None) -> dict[str, Any]:
    """Format HA tool to OpenAI/ModelScope compatible format."""
    tool_spec = {
        "type": "function",
        "function": {
            "name": tool.name,
            "description": tool.description or "",
        }
    }

    # Convert parameters schema if provided
    if tool.parameters and tool.parameters.schema:
        try:
            # Use voluptuous_openapi to convert schema
            schema = convert(tool.parameters, custom_serializer=custom_serializer)
            tool_spec["function"]["parameters"] = schema
            LOGGER.debug("Converted tool %s parameters: %s", tool.name, schema)
        except Exception as err:
            LOGGER.warning("Failed to convert parameters for tool %s: %s", tool.name, err)
            # Fallback to empty parameters
            tool_spec["function"]["parameters"] = {
                "type": "object",
                "properties": {},
            }

    return tool_spec
```

#### 2.3 ÈáçÂÜô `_async_handle_chat_log()` ÊñπÊ≥ï

**ÂÖ≥ÈîÆÁâπÊÄß**Ôºö
- ‰ªé `chat_log.llm_api` ÊèêÂèñÂ∑•ÂÖ∑
- ÂÆûÁé∞Â∑•ÂÖ∑Ë∞ÉÁî®Âæ™ÁéØÔºàÊúÄÂ§ö 10 Ê¨°Ëø≠‰ª£Ôºâ
- Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ËØ∑Ê±ÇÂíåÁªìÊûú
- Ê≠£Á°ÆÊûÑÈÄ† `ToolResultContent` ÂØπË±°
- Â§ÑÁêÜ VL Ê®°Âûã‰∏çÊîØÊåÅ function calling ÁöÑÊÉÖÂÜµ

```python
async def _async_handle_chat_log(
    self,
    chat_log: conversation.ChatLog,
    structure: dict[str, Any] | None = None,
) -> None:
    """Handle a chat log by calling the ModelScope API with function calling support."""

    # Get configuration
    model = self._get_option(CONF_CHAT_MODEL, RECOMMENDED_CHAT_MODEL)
    temperature = self._get_option(CONF_TEMPERATURE, DEFAULT_TEMPERATURE)
    top_p = self._get_option(CONF_TOP_P, DEFAULT_TOP_P)
    max_tokens = self._get_option(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)
    prompt = self._get_option(CONF_PROMPT, DEFAULT_PROMPT)

    # Extract tools from chat_log if available
    tools = None
    custom_serializer = llm.selector_serializer
    if chat_log.llm_api:
        tools = [
            _format_tool(tool, chat_log.llm_api.custom_serializer)
            for tool in chat_log.llm_api.tools
        ]
        custom_serializer = chat_log.llm_api.custom_serializer
        LOGGER.info("Extracted %d tools from chat_log.llm_api", len(tools))

    # Iterate up to MAX_TOOL_ITERATIONS to handle tool calls
    for iteration in range(MAX_TOOL_ITERATIONS):
        LOGGER.debug("Tool calling iteration %d/%d", iteration + 1, MAX_TOOL_ITERATIONS)

        # Prepare messages from chat_log
        messages = self._prepare_messages_from_chat_log(chat_log, prompt, structure, custom_serializer)

        # Call ModelScope API with tools
        response = await self.client.generate_text(
            model=model,
            messages=messages,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            tools=tools,
        )

        # Extract response
        choice = response["choices"][0]
        message = choice.get("message", {})

        # Check for tool calls
        tool_calls = message.get("tool_calls")
        content = message.get("content")
        finish_reason = choice.get("finish_reason")

        # Handle edge case: VL models that don't support function calling
        if finish_reason == "tool_calls" and not tool_calls:
            LOGGER.error(
                "Model returned finish_reason='tool_calls' but tool_calls is None. "
                "This usually means the model doesn't fully support function calling. "
                "Consider using Qwen/Qwen2.5-72B-Instruct instead of VL models."
            )
            assistant_content = conversation.AssistantContent(
                agent_id=self.entry.entry_id,
                content="Êä±Ê≠âÔºåÊàëÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢ò„ÄÇËØ∑Â∞ùËØïÂàáÊç¢Âà∞ Qwen/Qwen2.5-72B-Instruct Ê®°Âûã‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑËÆæÂ§áÊéßÂà∂ÊîØÊåÅ„ÄÇ"
            )
            chat_log.content.append(assistant_content)
            break

        # Add assistant message to chat_log
        if tool_calls:
            # Model wants to call tools
            LOGGER.info("Model requested %d tool calls", len(tool_calls))

            # Convert to HA ToolInput format
            ha_tool_calls = []
            for tool_call in tool_calls:
                function = tool_call.get("function", {})
                tool_name = function.get("name")
                tool_args = function.get("arguments", {})

                # Parse arguments if it's a string
                if isinstance(tool_args, str):
                    import json
                    try:
                        tool_args = json.loads(tool_args)
                    except json.JSONDecodeError as err:
                        LOGGER.error("Failed to parse tool arguments: %s", err)
                        tool_args = {}

                ha_tool_calls.append(
                    llm.ToolInput(tool_name=tool_name, tool_args=tool_args)
                )

            # Add assistant content with tool calls
            assistant_content = conversation.AssistantContent(
                agent_id=self.entry.entry_id,
                content=content or "",  # Content may be empty when calling tools
                tool_calls=ha_tool_calls,
            )
            chat_log.content.append(assistant_content)

            # Execute tools via HA's conversation system
            if chat_log.llm_api:
                for i, tool_call_input in enumerate(ha_tool_calls):
                    try:
                        # Get the tool_call_id from the original response
                        tool_call_id = tool_calls[i].get("id", f"call_{i}")

                        # Execute the tool
                        tool_result = await chat_log.llm_api.async_call_tool(tool_call_input)

                        # Add tool result to chat_log with required parameters
                        tool_result_content = conversation.ToolResultContent(
                            agent_id=self.entry.entry_id,      # ÂøÖÈúÄÂèÇÊï∞
                            tool_call_id=tool_call_id,          # ÂøÖÈúÄÂèÇÊï∞
                            tool_name=tool_call_input.tool_name,
                            tool_result=tool_result,
                        )
                        chat_log.content.append(tool_result_content)

                        LOGGER.debug("Tool %s executed successfully", tool_call_input.tool_name)

                    except Exception as err:
                        LOGGER.error("Error executing tool %s: %s", tool_call_input.tool_name, err)
                        # Add error result with required parameters
                        tool_call_id = tool_calls[i].get("id", f"call_{i}") if i < len(tool_calls) else f"call_{i}"
                        error_result = conversation.ToolResultContent(
                            agent_id=self.entry.entry_id,      # ÂøÖÈúÄÂèÇÊï∞
                            tool_call_id=tool_call_id,          # ÂøÖÈúÄÂèÇÊï∞
                            tool_name=tool_call_input.tool_name,
                            tool_result={"error": str(err)},
                        )
                        chat_log.content.append(error_result)

            # Continue loop to send tool results back to model
            continue

        else:
            # No tool calls, final response
            if content:
                assistant_content = conversation.AssistantContent(
                    agent_id=self.entry.entry_id,
                    content=content
                )
                chat_log.content.append(assistant_content)
                LOGGER.debug("Added final assistant response to chat_log")
            else:
                LOGGER.warning("Empty content in final response")

            # Done, exit loop
            break

    else:
        # Reached MAX_TOOL_ITERATIONS without finishing
        LOGGER.warning("Reached maximum tool iterations (%d), stopping", MAX_TOOL_ITERATIONS)
```

#### 2.4 Ê∑ªÂä†ËæÖÂä©ÊñπÊ≥ï

**`_prepare_messages_from_chat_log()`**ÔºöÂáÜÂ§áÂèëÈÄÅÁªô API ÁöÑÊ∂àÊÅØÂàóË°®

```python
def _prepare_messages_from_chat_log(
    self,
    chat_log: conversation.ChatLog,
    prompt: str | None,
    structure: dict[str, Any] | None,
    custom_serializer: Any,
) -> list[dict[str, Any]]:
    """Prepare messages list from chat_log content."""
    messages = []

    # Process chat_log content
    for content in chat_log.content:
        if isinstance(content, conversation.SystemContent):
            # Add system content from HA
            messages.append({"role": "system", "content": content.content})
            LOGGER.debug("Added SystemContent: %d chars", len(content.content))

        elif isinstance(content, conversation.UserContent):
            # Add user message
            if content.attachments:
                # Multi-modal content with images
                message_content = []
                if content.content:
                    message_content.append({"type": "text", "text": content.content})

                # Add image attachments
                for attachment in content.attachments:
                    import base64
                    try:
                        with open(attachment.path, 'rb') as img_file:
                            image_data = base64.b64encode(img_file.read()).decode('utf-8')
                            message_content.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{attachment.mime_type};base64,{image_data}"
                                }
                            })
                    except Exception as err:
                        LOGGER.error("Failed to read image %s: %s", attachment.path, err)

                messages.append({"role": "user", "content": message_content})
            else:
                # Simple text message
                messages.append({"role": "user", "content": content.content})

        elif isinstance(content, conversation.AssistantContent):
            # Add assistant message
            message = {"role": "assistant"}

            if content.content:
                message["content"] = content.content

            # Add tool calls if present
            if content.tool_calls:
                message["tool_calls"] = [
                    {
                        "id": f"call_{i}",
                        "type": "function",
                        "function": {
                            "name": tc.tool_name,
                            "arguments": tc.tool_args if isinstance(tc.tool_args, str) else str(tc.tool_args),
                        }
                    }
                    for i, tc in enumerate(content.tool_calls)
                ]

            messages.append(message)

        elif isinstance(content, conversation.ToolResultContent):
            # Add tool result as a tool message
            # OpenAI format expects tool messages with tool_call_id
            messages.append({
                "role": "tool",
                "name": content.tool_name,
                "content": str(content.tool_result) if content.tool_result else "{}",
            })
            LOGGER.debug("Added tool result for %s", content.tool_name)

    # Add custom prompt if no system content yet
    has_system_content = any(isinstance(c, conversation.SystemContent) for c in chat_log.content)
    if prompt and not has_system_content:
        messages.insert(0, {"role": "system", "content": prompt})

    # Add structure prompt if needed
    if structure:
        structure_prompt = self._format_structure_prompt(structure, custom_serializer)
        messages.append({"role": "system", "content": structure_prompt})

    return messages
```

### 3. `const.py` - Ê∑ªÂä†Ê®°ÂûãÈÄâÊã©ËØ¥Êòé

**Êñá‰ª∂**Ôºö`custom_components/yanfeng_ai_task/const.py`

**Êõ¥Êîπ**ÔºöÊ∑ªÂä†ÂÖ≥‰∫éÊ®°ÂûãÈÄâÊã©ÁöÑÊ≥®Èáä

```python
# Recommended models
# Note: Use pure text models for function calling, not VL (Vision-Language) models
RECOMMENDED_CHAT_MODEL = "Qwen/Qwen2.5-72B-Instruct"  # Best for function calling
RECOMMENDED_IMAGE_MODEL = "Qwen/Qwen-Image"
```

**ÈáçË¶ÅËØ¥Êòé**Ôºö
- ‚úÖ **Êé®Ëçê‰ΩøÁî®Á∫ØÊñáÊú¨Ê®°Âûã**Ôºö`Qwen/Qwen2.5-72B-Instruct`„ÄÅ`Qwen/Qwen2.5-32B-Instruct` Á≠â
- ‚ùå **‰∏çÊé®Ëçê VL Ê®°ÂûãÁî®‰∫éËÆæÂ§áÊéßÂà∂**Ôºö`Qwen/Qwen3-VL-235B-A22B-Instruct` Á≠âËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂØπÁ∫ØÊñáÊú¨ÁöÑ function calling ÊîØÊåÅ‰∏çÂÆåÊï¥

## üß™ ÊµãËØïÈ™åËØÅ

### ÊµãËØïËÑöÊú¨

ÂàõÂª∫‰∫Ü `test_function_calling.py` ËÑöÊú¨Áî®‰∫éÈ™åËØÅ ModelScope API ÁöÑ function calling ÊîØÊåÅ„ÄÇ

**ËøêË°åÊñπÊ≥ï**Ôºö
```bash
# ÊñπÊ≥ï 1Ôºö‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáè
export MODELSCOPE_API_KEY='your-api-key'
python test_function_calling.py

# ÊñπÊ≥ï 2ÔºöÂëΩ‰ª§Ë°åÂèÇÊï∞
python test_function_calling.py your-api-key
```

**ÊµãËØïÂÜÖÂÆπ**Ôºö
1. ÊôÆÈÄöË∞ÉÁî®Ôºà‰∏çÂ∏¶Â∑•ÂÖ∑Ôºâ
2. Â∏¶Â∑•ÂÖ∑Ë∞ÉÁî®ÔºàFunction CallingÔºâ
3. ÂèëÈÄÅÂ∑•ÂÖ∑ÊâßË°åÁªìÊûúÂõûÊ®°Âûã

**È¢ÑÊúüÁªìÊûú**Ôºö
```
‚úÖ ÊµãËØïÈÄöËøá! ModelScope API ÊîØÊåÅ Function Calling!

Â∑•ÂÖ∑Ë∞ÉÁî®ÊµÅÁ®ãÔºö
1. Áî®Êà∑ÈóÆÈ¢òÔºöÁé∞Âú®‰π¶ÊàøÁöÑÊ∏©Â∫¶ÊòØÂ§öÂ∞ëÔºü
2. Ê®°ÂûãËøîÂõûÔºötool_call { name: "get_temperature", arguments: {"room": "‰π¶Êàø"} }
3. ÊâßË°åÂ∑•ÂÖ∑ÔºöËøîÂõûÊ∏©Â∫¶Êï∞ÊçÆ {"temperature": 30.8, "unit": "¬∞C"}
4. Ê®°ÂûãÊúÄÁªàÂõûÂ§çÔºö‰π¶ÊàøÁöÑÊ∏©Â∫¶Áé∞Âú®ÊòØ 30.8¬∞C
```

### ÈõÜÊàêÊµãËØï

**ÊµãËØïÁéØÂ¢É**ÔºöHome Assistant

**ÊµãËØïÊ≠•È™§**Ôºö

1. **ÈÖçÁΩÆÈõÜÊàê**
   - ËøõÂÖ• **ËÆæÁΩÆ > ËÆæÂ§á‰∏éÊúçÂä°**
   - ÈÖçÁΩÆ **Yanfeng AI Task** ÈõÜÊàê
   - ÈÖçÁΩÆÂ≠êÈ°π **Yanfeng AI Conversation**
   - **ÂÖ≥ÈîÆÈÖçÁΩÆ**Ôºö
     - Ê®°ÂûãÈÄâÊã©Ôºö`Qwen/Qwen2.5-72B-Instruct`Ôºà‰∏çË¶ÅÁî® VL Ê®°ÂûãÔºâ
     - LLM Hass APIÔºöÈÄâÊã© `Assist`ÔºàÂêØÁî®ËÆæÂ§áÊéßÂà∂Ôºâ

2. **ÈáçÂêØ Home Assistant**
   ```bash
   # Á°Æ‰øù‰ª£Á†ÅÊõ¥Êñ∞ÁîüÊïà
   ```

3. **ÂêØÁî®Ë∞ÉËØïÊó•Âøó**
   Âú® `configuration.yaml` ‰∏≠Ê∑ªÂä†Ôºö
   ```yaml
   logger:
     default: info
     logs:
       custom_components.yanfeng_ai_task: debug
       custom_components.yanfeng_ai_task.entity: debug
       homeassistant.components.conversation: debug
   ```

4. **ÊµãËØïÊü•ËØ¢**
   Âú® HA ÁöÑÂØπËØùÁïåÈù¢ËæìÂÖ•Ôºö
   - "Áé∞Âú®‰π¶ÊàøÊ∏©Â∫¶Â§öÂ∞ëÔºü"
   - "ÂÆ¢ÂéÖÁÅØÊòØÂºÄÁùÄÁöÑÂêóÔºü"
   - "Á©∫Ë∞ÉÁé∞Âú®ËÆæÁΩÆÁöÑÊòØ‰ªÄ‰πàÊ∏©Â∫¶Ôºü"

**È¢ÑÊúüÊó•ÂøóËæìÂá∫**Ôºö
```
INFO (MainThread) [custom_components.yanfeng_ai_task.entity] Extracted 22 tools from chat_log.llm_api
DEBUG (MainThread) [custom_components.yanfeng_ai_task.entity] Tool calling iteration 1/10
DEBUG (MainThread) [custom_components.yanfeng_ai_task.entity] Sending 3 messages to ModelScope (iteration 1)
INFO (MainThread) [custom_components.yanfeng_ai_task.entity] Model requested 1 tool calls
DEBUG (MainThread) [custom_components.yanfeng_ai_task.entity] Tool GetLiveContext executed successfully
DEBUG (MainThread) [custom_components.yanfeng_ai_task.entity] Tool calling iteration 2/10
DEBUG (MainThread) [custom_components.yanfeng_ai_task.entity] Added final assistant response to chat_log
```

**ÊàêÂäüÊ†áÂøó**Ôºö
- ‚úÖ ÁúãÂà∞ "Extracted X tools from chat_log.llm_api"
- ‚úÖ ÁúãÂà∞ "Model requested X tool calls"
- ‚úÖ ÁúãÂà∞ "Tool GetLiveContext executed successfully"
- ‚úÖ Êî∂Âà∞ÂåÖÂê´ËÆæÂ§á‰ø°ÊÅØÁöÑÂõûÂ§ç

## üêõ Â∏∏ËßÅÈóÆÈ¢òÊéíÊü•

### ÈóÆÈ¢ò 1ÔºöÂ∑•ÂÖ∑Ê≤°ÊúâË¢´ÊèêÂèñ

**ÁóáÁä∂**ÔºöÊó•Âøó‰∏≠Ê≤°Êúâ "Extracted X tools from chat_log.llm_api"

**ÂéüÂõ†**Ôºö
- `CONF_LLM_HASS_API` Êú™ÂêØÁî®
- `chat_log.llm_api` ‰∏∫ None

**Ëß£ÂÜ≥ÊñπÊ°à**Ôºö
1. Ê£ÄÊü•ÈÖçÁΩÆÔºöÁ°Æ‰øùÂú® Conversation Â≠êÈÖçÁΩÆ‰∏≠ÂêØÁî®‰∫Ü "LLM Hass API"
2. ÈáçÂêØ Home Assistant
3. Êü•Áúã `conversation.py` ÁöÑÊó•ÂøóÁ°ÆËÆ§ÈÖçÁΩÆÊ≠£Á°Æ

### ÈóÆÈ¢ò 2ÔºöÊ®°Âûã‰∏çË∞ÉÁî®Â∑•ÂÖ∑

**ÁóáÁä∂**ÔºöÊúâÂ∑•ÂÖ∑ÂÆö‰πâÔºå‰ΩÜÊ®°ÂûãÁõ¥Êé•ÂõûÁ≠îËÄå‰∏çË∞ÉÁî®Â∑•ÂÖ∑

**ÂèØËÉΩÂéüÂõ†**Ôºö
1. ‰ΩøÁî®‰∫Ü VL Ê®°ÂûãÔºà‰∏çÊîØÊåÅÁ∫ØÊñáÊú¨ function callingÔºâ
2. Á≥ªÁªüÊèêÁ§∫ËØçÊ≤°ÊúâÂºïÂØºÊ®°Âûã‰ΩøÁî®Â∑•ÂÖ∑
3. Áî®Êà∑ÈóÆÈ¢ò‰∏çÈúÄË¶ÅÂ∑•ÂÖ∑

**Ëß£ÂÜ≥ÊñπÊ°à**Ôºö
1. ÂàáÊç¢Âà∞Á∫ØÊñáÊú¨Ê®°ÂûãÔºö`Qwen/Qwen2.5-72B-Instruct`
2. Ê£ÄÊü•Á≥ªÁªüÊèêÁ§∫ËØçÊòØÂê¶ÊòéÁ°ÆÊåáÁ§∫‰ΩøÁî®Â∑•ÂÖ∑
3. Â∞ùËØïÊõ¥ÊòéÁ°ÆÁöÑËÆæÂ§áÊü•ËØ¢ÈóÆÈ¢ò

### ÈóÆÈ¢ò 3ÔºöToolResultContent ÂèÇÊï∞ÈîôËØØ

**ÁóáÁä∂**Ôºö
```
TypeError: ToolResultContent.__init__() missing 2 required positional arguments: 'agent_id' and 'tool_call_id'
```

**ÂéüÂõ†**ÔºöÊóßÁâàÊú¨‰ª£Á†ÅÊ≤°ÊúâÊèê‰æõÂøÖÈúÄÂèÇÊï∞

**Ëß£ÂÜ≥ÊñπÊ°à**ÔºöÁ°Æ‰øù‰ΩøÁî®ÊúÄÊñ∞ÁâàÊú¨ÁöÑ `entity.py`ÔºåÂåÖÂê´‰ª•‰∏ã‰øÆÂ§çÔºö
```python
tool_result_content = conversation.ToolResultContent(
    agent_id=self.entry.entry_id,      # ÂøÖÈúÄ
    tool_call_id=tool_call_id,          # ÂøÖÈúÄ
    tool_name=tool_call_input.tool_name,
    tool_result=tool_result,
)
```

### ÈóÆÈ¢ò 4ÔºöVL Ê®°ÂûãËøîÂõû finish_reason='tool_calls' ‰ΩÜ tool_calls=None

**ÁóáÁä∂**Ôºö
```
finish_reason: 'tool_calls'
tool_calls: None
```

**ÂéüÂõ†**ÔºöVLÔºàVision-LanguageÔºâÊ®°ÂûãÂØπÁ∫ØÊñáÊú¨ÁöÑ function calling ÊîØÊåÅ‰∏çÂÆåÊï¥

**Ëß£ÂÜ≥ÊñπÊ°à**Ôºö
1. ÂàáÊç¢Âà∞Á∫ØÊñáÊú¨Ê®°ÂûãÔºö`Qwen/Qwen2.5-72B-Instruct`
2. ‰ª£Á†ÅÂ∑≤ÂåÖÂê´Ê≠§ÊÉÖÂÜµÁöÑÈîôËØØÂ§ÑÁêÜÔºå‰ºöÊèêÁ§∫Áî®Êà∑ÂàáÊç¢Ê®°Âûã

## üìä ÊÄßËÉΩËÄÉËôë

### Â∑•ÂÖ∑Ë∞ÉÁî®Âª∂Ëøü

**ÂÖ∏ÂûãÂª∂Ëøü**Ôºö
- Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî®ÔºàÂèëÈÄÅÂ∑•ÂÖ∑Ë∞ÉÁî®ËØ∑Ê±ÇÔºâÔºö500-1500ms
- Â∑•ÂÖ∑ÊâßË°åÔºö50-200msÔºàÂèñÂÜ≥‰∫éÂ∑•ÂÖ∑Á±ªÂûãÔºâ
- Á¨¨‰∫åÊ¨°Ë∞ÉÁî®ÔºàÂèëÈÄÅÂ∑•ÂÖ∑ÁªìÊûúÔºåËé∑ÂèñÊúÄÁªàÂõûÂ§çÔºâÔºö500-1500ms
- **ÊÄªËÆ°**ÔºöÁ∫¶ 1-3 Áßí

### Ëø≠‰ª£ÈôêÂà∂

ËÆæÁΩÆ‰∫Ü `MAX_TOOL_ITERATIONS = 10` Èò≤Ê≠¢Êó†ÈôêÂæ™ÁéØÔºö
- Ê≠£Â∏∏ÊÉÖÂÜµÔºö1-3 Ê¨°Ëø≠‰ª£ÔºàÂàùÂßãËØ∑Ê±Ç ‚Üí Â∑•ÂÖ∑Ë∞ÉÁî® ‚Üí ÊúÄÁªàÂõûÂ§çÔºâ
- Â§çÊùÇ‰ªªÂä°Ôºö3-5 Ê¨°Ëø≠‰ª£ÔºàÂ§ö‰∏™Â∑•ÂÖ∑Ë∞ÉÁî®Ôºâ
- ËææÂà∞‰∏äÈôêÔºöËÆ∞ÂΩïË≠¶ÂëäÊó•ÂøóÂπ∂ÂÅúÊ≠¢

## üîÑ Â∑•‰ΩúÊµÅÁ®ãÂõæ

```
Áî®Êà∑ÈóÆÈ¢òÔºö"‰π¶ÊàøÁé∞Âú®Â§öÂ∞ëÂ∫¶Ôºü"
    ‚Üì
conversation.py: _async_handle_message()
    ‚Üì
chat_log.async_provide_llm_data() ‚Üí Êèê‰æõÂ∑•ÂÖ∑ÂÆö‰πâÂíå‰∏ä‰∏ãÊñá
    ‚Üì
entity.py: _async_handle_chat_log()
    ‚Üì
ÊèêÂèñÂ∑•ÂÖ∑ ‚Üí Ê†ºÂºèÂåñ‰∏∫ OpenAI Ê†ºÂºè
    ‚Üì
„ÄêÁ¨¨1Ê¨°Ëø≠‰ª£„Äë
    ‚Üì
ÂáÜÂ§áÊ∂àÊÅØ ‚Üí Ë∞ÉÁî® ModelScope APIÔºàÂ∏¶Â∑•ÂÖ∑ÂÆö‰πâÔºâ
    ‚Üì
Ê®°ÂûãÂìçÂ∫î: tool_calls = [{"function": {"name": "GetLiveContext", "arguments": {...}}}]
    ‚Üì
ÊâßË°åÂ∑•ÂÖ∑ ‚Üí chat_log.llm_api.async_call_tool()
    ‚Üì
ÂàõÂª∫ ToolResultContent ‚Üí Ê∑ªÂä†Âà∞ chat_log
    ‚Üì
„ÄêÁ¨¨2Ê¨°Ëø≠‰ª£„Äë
    ‚Üì
ÂáÜÂ§áÊ∂àÊÅØÔºàÂåÖÂê´Â∑•ÂÖ∑ÁªìÊûúÔºâ‚Üí Ë∞ÉÁî® ModelScope API
    ‚Üì
Ê®°ÂûãÂìçÂ∫î: content = "‰π¶ÊàøÁöÑÊ∏©Â∫¶Áé∞Âú®ÊòØ 24.5¬∞C"
    ‚Üì
ÂàõÂª∫ AssistantContent ‚Üí Ê∑ªÂä†Âà∞ chat_log
    ‚Üì
ËøîÂõûÊúÄÁªàÁªìÊûúÁªôÁî®Êà∑
```

## ‚úÖ È™åÊî∂Ê†áÂáÜ

ÈõÜÊàêÂäüËÉΩÊ≠£Â∏∏ÁöÑÊ†áÂøóÔºö

1. **Â∑•ÂÖ∑ÊèêÂèñÊàêÂäü**
   - Êó•ÂøóÊòæÁ§∫Ôºö`Extracted 22 tools from chat_log.llm_api`

2. **Ê®°ÂûãËØ∑Ê±ÇÂ∑•ÂÖ∑Ë∞ÉÁî®**
   - Êó•ÂøóÊòæÁ§∫Ôºö`Model requested 1 tool calls`
   - Êó•ÂøóÊòæÁ§∫Â∑•ÂÖ∑ÂêçÁß∞ÂíåÂèÇÊï∞

3. **Â∑•ÂÖ∑ÊâßË°åÊàêÂäü**
   - Êó•ÂøóÊòæÁ§∫Ôºö`Tool GetLiveContext executed successfully`
   - Ê≤°ÊúâÂºÇÂ∏∏ÊàñÈîôËØØ

4. **Ëé∑ÂæóÊúÄÁªàÂõûÂ§ç**
   - Êó•ÂøóÊòæÁ§∫Ôºö`Added final assistant response to chat_log`
   - Áî®Êà∑Êî∂Âà∞ÂåÖÂê´ËÆæÂ§á‰ø°ÊÅØÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂõûÂ§ç

5. **Áî®Êà∑‰ΩìÈ™å**
   - ÂõûÂ§çÈÄüÂ∫¶Ôºö1-3 ÁßíÔºàÂèØÊé•ÂèóÔºâ
   - ÂõûÂ§çÂáÜÁ°ÆÔºöÂåÖÂê´Ê≠£Á°ÆÁöÑËÆæÂ§áÁä∂ÊÄÅ‰ø°ÊÅØ
   - ÂõûÂ§çËá™ÁÑ∂Ôºö‰ΩøÁî®‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄË°®Ëææ

## üìö ÂèÇËÄÉËµÑÊñô

### Home Assistant ÊñáÊ°£
- [LLM API ÂºÄÂèëÊñáÊ°£](https://developers.home-assistant.io/docs/core/llm/)
- [Conversation ÈõÜÊàêÊñáÊ°£](https://www.home-assistant.io/integrations/conversation/)

### ModelScope ÊñáÊ°£
- [API ÊñáÊ°£](https://modelscope.cn/docs)
- [Qwen Ê®°ÂûãÊñáÊ°£](https://github.com/QwenLM/Qwen)

### OpenAI Function Calling
- [Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)

### ÂèÇËÄÉÂÆûÁé∞
- Google Generative AI ÈõÜÊàêÔºàHome Assistant ÂÆòÊñπÔºâ
- Êú¨È°πÁõÆÁöÑ `test_function_calling.py` ÊµãËØïËÑöÊú¨

## üéâ ÊÄªÁªì

ÈÄöËøáÊú¨Ê¨°ÂÆûÁé∞Ôºö

1. ‚úÖ **ÂÆåÂÖ®ÂÆûÁé∞‰∫Ü OpenAI ÂÖºÂÆπÁöÑ Function Calling**
2. ‚úÖ **ÊîØÊåÅÊâÄÊúâ Home Assistant Êèê‰æõÁöÑÂ∑•ÂÖ∑**ÔºàGetLiveContext„ÄÅControlDevice Á≠âÔºâ
3. ‚úÖ **Ê≠£Á°ÆÂ§ÑÁêÜÂ§öËΩÆÂ∑•ÂÖ∑Ë∞ÉÁî®**
4. ‚úÖ **Êèê‰æõËØ¶ÁªÜÁöÑË∞ÉËØïÊó•Âøó**
5. ‚úÖ **Â§ÑÁêÜÈîôËØØÊÉÖÂÜµ**ÔºàVL Ê®°Âûã„ÄÅÂ∑•ÂÖ∑ÊâßË°åÂ§±Ë¥•Á≠âÔºâ

Áé∞Âú® Yanfeng AI Task ÈõÜÊàêÂèØ‰ª•ÂÉè Gemini ‰∏ÄÊ†∑Êü•ËØ¢ÂíåÊéßÂà∂ Home Assistant ËÆæÂ§á‰∫ÜÔºÅ

---

**ÊúÄÂêéÊõ¥Êñ∞**Ôºö2025-01-17
**ÁâàÊú¨**Ôºöv2.0.0
**Áä∂ÊÄÅ**Ôºö‚úÖ Â∑≤ÂÆåÊàêÂπ∂ÊµãËØï
